# METHODS: THE PROPOSED FRAMEWORK {#sec:approach}

We present the steps of a proposed framework for selecting MPs based on trade-offs in performance amongst MPs.
Sections \@ref(sec:approach1) to \@ref(sec:approach6) describe the methods of the framework, as it would be applied in the provision of catch advice, organized according to the six best-practice steps described in Section \@ref(sec:best-practices).
We note that the elements of the framework as laid out here, represent primarily the role of Science.
When this framework is applied in provision of management advice, decision-makers, stakeholders, and other interested parties (e.g., First Nations, Nongovernmental Organizations [NGOs], and academics) should be engaged throughout the process, particularly in defining the decision context, setting objectives and performance metrics, and selecting MPs [e.g., @cox2008a].

## STEP 1: DEFINE THE DECISION CONTEXT {#sec:approach1}

For quota-managed groundfish species in BC, the decision to be made is which MP to use to determine catch limits for the period until the next available catch advice.
The time frame for making the decision should be stated in the request for science advice. The boundaries on the project should be decided by a "technical committee", convened for each assessment, and typically comprised of representatives from DFO Science, Fisheries Management, First Nations, commercial and recreational fishing representatives, NGOs and other interested parties, as required.
Examples of project elements to be scoped include key uncertainties to be included and excluded in the OMs, data to be included and excluded, and explicit trade-offs to be considered.
These will be discussed in more detail in the following sections.

The final decision on which MP to use to determine catch limits should be made based on a consensus by the Regional Peer Review committee, after review of the scientific content of the advice (including the structure and content of the OMs), and consideration of the relative performance of the MPs with respect to meeting stated objectives and trade-offs among performance metrics.
The Regional Peer Review committee will typically be comprised of the technical committee plus a much broader range of interested parties representing DFO Science, Fisheries Management, First Nations, commercial and recreational fishing representatives, NGOs and other interested parties.

The simulation framework tests the performance of specific MPs and ultimately a single catch limit from the final selected MP.
The framework does not test posthoc adjustments to the catch limit recommended by an MP.
This is in contrast to the decision tables presented in most Pacific Region groundfish stock assessments, where a range of possible catch limits with forecast of future stock status under each catch limit is provided for decision-making.
Regardless of the framework used for advice, we note that it remains the purview of the Fisheries Minister to make posthoc adjustments to catch limits, based on cultural, social, or economic considerations, in accordance with the *Fisheries Act* (Sections 6.1 (2) and 6.2(2))

## STEP 2: SELECTION OF OBJECTIVES AND PERFORMANCE METRICS  {#sec:approach2}

Here we describe a set of provisional objectives and associated performance metrics, refined after discussions with our technical advisory group.
In applications of the framework, objectives and performance metrics should be refined on a stock-by-stock basis, with advice from Fisheries Management, First Nations, commercial and recreational fishing representatives, NGOs and other affected parties.
Other objectives and performance metrics could be added (e.g., cultural objectives), decided on a stock-by-stock basis.
The time-frame of interest for setting objectives and calculating performance metrics may also be determined on a stock-by-stock basis, as trade-offs change over time [@cox2008a]. 

Key provisional conservation objectives are guided by the Precautionary Approach Framework [@dfo2006; @dfo2009], elements of which are incorporated into the Fish Stocks provisions of the *Fisheries Act* (see Section \@ref(sec:intro-motivation)).
Additional objectives related to fisheries catch and variability in annual fisheries catch are based on precedents in other DFO Pacific Region analyses [e.g., @cox2008a; @forrest2018; @cox2019].
 
We propose the following provisional tactical conservation and fisheries objectives:

1. Maintain stock status above the LRP in the long term with high probability.
2. Maintain stock status above the USR in the long term with some probability.
3. Maintain a fishing exploitation rate below the rate at maximum sustainable yield with some probability.
4. Given the above conservation objectives are achieved, maximize short- and long-term fisheries catch.
5. Given the above conservation objectives are achieved, minimize variability in fisheries catch from year to year.

Objective 1 is implicit in Section 6.1 (1) of the *Fisheries Act* and explicit in Section 6.1(2) (see Section \@ref(sec:intro-motivation) of this document).
Objectives 2 and 3 are interpretations of Section 6.1 (1) of the *Fisheries Act*, where it is stated that "the Minister shall implement measures to maintain major fish stocks at or above the level necessary to promote the sustainability of the stock".
The term sustainability has many definitions as they pertain to fisheries [@hilborn2015; @marentette2020a].
DFO Science is currently evaluating the language of the Fish Stocks provisions to clarify definitions of sustainability with respect to managing Canadian fisheries [@marentette2020b].
Here, we assume that the objective of maintaining the stock in the Healthy Zone (Figure \@ref(fig:pa-illustration)) with non-zero probability is consistent with the language of maintaining stocks at or above levels to promote sustainability.
We use provisional values of LRP = 0.4 *B*~MSY~ and USR = 0.8 *B*~MSY~, suggested under the Precautionary Approach Framework [@dfo2009] (we herein use *B* to refer to spawning biomass).
See @marentette2020c and @marentette2020b for discussion of current evolving thinking on the role of these reference points in Canadian policy and legislation.

The specific probabilities assigned to successfully achieving each objective likely need consideration on a stock-by-stock basis.
For Objective 1, international best practice suggests the probability of maintaining stocks above the LRP should be 90--95% [@sainsbury2008; @mcilgorm2013; @ices2018; @marentette2020a], while the probability of reaching a target biomass (e.g., the threshold to the Healthy Zone or some pre-defined target above the USR) can be lower at around 50% [@mcilgorm2013].

We propose the following provisional performance metrics, where MSY refers to maximum sustainable yield, *B*~MSY~ refers to equilibrium spawning biomass at MSY, and *F*~MSY~ refers to the fishing mortality that produces MSY over the long term:

1. LT LRP: Probability *B* > 0.4 *B*~MSY~ (long-term year range)
2. LT USR: Probability *B* > 0.8 *B*~MSY~ (long-term year range)
3. FMSY: P(*F* < *F*~MSY~) (entire projection)
4. STC: Probability catch > reference catch (years 1--10)
5. LTC: Probability catch > reference catch (long-term year range)
6. AADC: Probability AADC (average absolute interannual difference in catch) < historical AADC (entire projection)

In the above list of performance metrics, LT LRP and LT USR are conservation metrics measuring Objectives 1 and 2 over the long term.
FMSY is a conservation performance metric measuring Objective 3 over the whole projection period.
LTC and STC are economic metrics, representing Objective 4, measured in the short and long-term, respectively.
AADC is an economic metric, representing Objective 5, measured over the whole projection period.
Variability in catch, represented by AADC, is likely to be more important for target species than for incidentally-caught, and likely data-limited, species.
However, large fluctuations in incidentally-caught species may represent a problem in multispecies fisheries, where sudden increases in abundance of low-quota species co-occurring with target species may limit the ability of the fishery to realise TACs of target species.

The "long-term" year range should be defined on a stock-by-stock basis recognizing that shorter-lived stocks could use a shorter projection period, while longer-lived stocks may require a longer projection period.
However, barring some other specified requirement for a definition of long-term (e.g., an associated Committee on the Status of Endangered Wildlife in Canada [COSEWIC] process), we suggest considering "long-term" as the minimum of 1.5--2 generation times of the species or 50 years.
We further suggest averaging the long-term performance metrics over a short (e.g., 5--15 year) window before the final year.

We suggest the "short-term" year range, which only applies to the economic performance metric STC, reflect some time period that is of near-term interest to current participants in the fishery.
Our suggestion of 1--10 years represents a starting point and could be modified for specific fisheries.

The catch objectives STC and LTC are provisionally defined in terms of some reference catch that is deemed necessary or desirable for economic reasons to maintain the fishery in question or to maintain the multispecies BC groundfish fishery as a whole.
This reference level could be obtained through consultation with stakeholders or by identifying the average or minimum catch in recent years (e.g., minimum catch in the last five years).

The provisional performance metric AADC represents average absolute interannual difference in catch.
We chose this specific representation of variability to be easy to understand and simple to compare historical values.
It is calculated by subtracting each year's catch from the previous year's catch and taking the mean of the absolute value of these numbers.
In other words, it represents the expected deviation in catch from year to year.
Alternatives such as standard deviations in log space or coefficients of variation have a more abstract meaning and have been found to be challenging for some stakeholders to interpret [@punt2017].

Performance metrics in the MP Framework are easily customizable.
Performance metrics for applications of the framework may, for example, reflect a broader range of objectives such as those associated with rebuilding plans or COSEWIC criteria [e.g., @haggarty2020yelloweye].

In cases where performance metrics are calculated over a range of years, care needs to be taken to clearly report how summary statistics are calculated.
Provisionally, we suggest calculating performance statistics across replicates and the entire time window as defined for the performance metric.
For example, we calculated the *F*~MSY~ performance metric across all replicates and years simultaneously.
Alternative calculations would include calculating the performance statistics for a specific year of interest, calculating the proportion of years in which the performance metric was met [@australian_government2018], ensuring that the performance metric threshold is met in each and every year [@ices2016criteria], or each and every iteration.
Best practices for these calculations vary across jurisdictions [@marentette2020a] and different methods may imply different risk tolerances.
For example, requiring that a threshold is met in each and every year as in [@ices2016criteria] is a stricter requirement than requiring a threshold to be met on average over iterations and years.
Therefore, the method for calculating the performance metrics may need to be determined iteratively, in consultation with managers, stakeholders and First Nations. 

## STEP 3: SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS {#sec:approach3}

DLMtool OMs are organized into four main components representing a real fished system:

1. population dynamics of the fish stock (e.g., growth, recruitment, mortality);
2. fishery dynamics (e.g., selectivity, spatial targeting);
3. observation processes (e.g., bias and precision in survey indices); and
4. management implementation (e.g., percentage overages of catch limits).

Parameters under the four components are entered into "slots" [terminology referring to a feature of the "S4" object-oriented programming system in R; @r2019], described in detail in our Appendix \@ref(app:dlmtool-om) and in Appendix B of @carruthers2018.

DLMtool allows the incorporation of uncertainty in most OM parameters through optional specification of a distribution.
See Appendix B of @carruthers2018 for a full list of parameters for which a distribution can be specified.
To isolate the effects of specific sources of uncertainty on performance of MPs, we recommend development of alternative OMs that change the value (or distribution) of one or more parameters and/or data sources of interest [@punt2016].
In general, we recommend developing more than one OM, dividing OMs into a reference set of core OMs representing the most important plausible model uncertainties, and a robustness set for testing sensitivity to a broader range of structural uncertainties [@rademeyer2007].
We do not attempt to weight OMs in this framework.
For BC groundfish stocks, reference set uncertainties will likely be based on key uncertainties identified in typical groundfish stock assessments, namely natural mortality (*M*), steepness of the stock-recruit relationship (*h*), and initial depletion (i.e., depletion from an unfished state at the beginning of the projection period).
For stocks with available stock assessments, these should be consulted to identify major sources of uncertainty.

Candidate uncertainties to include in OMs in the robustness set may include:

* Changes in predation rate (e.g., seal predation)
* Changes in availability of prey
* The effectiveness of or changes to closed areas such as Rockfish Conservation Areas (RCAs)
* Alternative representations of survey and commercial fleet size selectivity
* Alternative catch histories (e.g., for species such as rockfishes, which were historically reported under generic species names)
* Implementation error (actual catches are above or below the TAC)

In some cases, these uncertainties may be included in the reference set.
In general an iterative approach may be required to reduce the robustness down to a set of OMs that provide contrast to results obtained with reference set OMs [@rademeyer2007].
We discuss recommendations for treatment and presentation of results from the reference and robustness sets in Section \@ref(sec:approach6).
To ensure transparency and reproducibility, we recommend that the full specification of OM parameters be clearly documented in appendices attached to the working paper and the code to accomplish the simulations be version controlled and archived on publication.

We note that DLMtool OMs include a large number of parameters that can vary through time, or which can be set to be deliberately biased.
To simplify the OM and focus on what are likely the most important axes of uncertainty, we suggest fixing most parameters to be time invariant and unbiased (Appendix \@ref(app:default-slots)).
Exceptions would occur when one or more of these parameters represent axes of uncertainty for specific stocks, or when certain time-varying parameters are already accepted components of the stock assessment.

Best practice recommends conditioning OMs with observed data so they can reproduce historical observations (e.g., indices of abundance, age-composition data).
DLMtool's companion software package, MSEtool [@huynh_msetool_2019], includes an efficient implementation of a stock reduction analysis (SRA) [@kimura1982; @walters2006] to help with this process (Appendix \@ref(app:sra)).
An SRA is effectively a catch-at-age model that estimates the combinations of unfished recruitment, depletion, fishing effort, recruitment deviations, and selectivity that would be consistent with the observed data given assumptions about other parameters (e.g., growth, natural mortality).
The SRA is run *n* times, with the number of replicates *n* matching that for the closed-loop simulations.
The SRA draws from parameter ranges specified in the OM (e.g., Appendix \@ref(app:desc-om-rex)), estimates key parameters and updates those parameters in the OM (see Appendix \@ref(app:dlmtool-om) for details).
SRA replicates that do not converge (with convergence defined as a positive-definite covariance matrix) are discarded.
The SRA implementation in MSEtool can be conditioned on catch or effort time series (Appendix \@ref(app:sra)).
For most applications for BC groundfish stocks, we suggest conditioning on catch, as historical trajectories of catch tend to be more reliable than time series of effort, especially given uncertainties in how to best represent and interpret effort in multispecies fisheries.
Further details on the SRA OM-conditioning model are provided in Appendix \@ref(app:sra).

For some data-limited stocks, indices of abundance may be considered less reliable due to sampling difficulties or rarity.
In these cases, as long as there is a time series of catch data, we still recommend using the SRA model for conditioning, recognizing that a much broader range of uncertainties will need to be considered in the set of OMs, including large uncertainty in stock size, productivity and current depletion level.
Where there is no index of abundance, we recommend developing a wide range of OMs conditioned on available catch data, which differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

We recognize that, for some data-limited stocks, there may be some efficiency gains in developing more generic OMs that capture the major biological, fleet, and observation characteristics of a set of similar stocks.
However, we recommend focusing first on species for which customized OMs can be developed, which are conditioned on observed data.

## STEP 4: IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES  {#sec:approach4}

We screened all MPs available in DLMtool as of November 2019 along with recent MPs used in BC groundfish reports to consider their appropriateness for the framework.
This represents a fairly comprehensive set of data-limited MPs available in the primary literature or agency reports to date.
Here, we describe the types of MPs available and the process by which we identified a provisional set of MPs, then we explain how some MPs were tailored to the BC groundfish needs.
We describe provisional candidate MPs in detail in Appendix \@ref(app:MPs).

DLMtool includes MPs that make many different kinds of management recommendations.
These recommendations include adjustments to total allowable catch (TAC), effort, or spatial allocation of catch or effort.
For the framework, we focus on MPs that make TAC recommendations because BC groundfish are managed in general by quotas.
Therefore, all MPs considered here take some subset of the data generated by an OM and provide a recommended catch for the subsequent year.

We focus on two main types of MPs: empirical and model-based MPs.
In choosing from the available empirical and model-based MPs, we excluded MPs based on a number of requirements that would rarely be met for our stocks.
We excluded MPs that required knowledge of absolute abundance since there are unlikely to be cases where we have such knowledge in a data-limited case.
We excluded MPs that required recent age composition data because we intend this framework to be applied to stocks for which recent age-composition data are not available.
We excluded MPs that required knowledge of depletion and steepness of the stock-recruit relationship since these are likely to be major axes of uncertainty for the stocks to which this framework will be applied.
While it is necessary to explore these axes of uncertainty within the OM, implementing an MP on real data when that MP requires knowledge of depletion and steepness would require additional assumptions.
In a few cases, we excluded MPs that we found difficult to communicate [e.g., MPs based on the value of the index relative to the time series mean and standard error, @jardim2015].
Such MPs did not perform appreciably differently from included MPs and we felt their exclusion would not lead to a loss in overall performance of the framework.

A library of provisional MPs included in this framework is described in Appendix \@ref(app:MPs).

### Empirical MPs

Commercial catch data are available for all BC groundfish stocks with relative certainty since 1996 for BC trawl fisheries and 2008 for BC hook-and-line fisheries.
Fisheries-independent trawl and longline surveys have been conducted systematically since the early 2000s for BC groundfish and the population indices derived from these data likely represent some of the most informative data for many data-limited groundfish stocks in BC.
Fish lengths are collected on both surveys and commercial fishing trips for many species.
However, length-based MPs often require strong assumptions and require that the simulated length-composition data are sufficiently "messy" to reflect the real-world length-composition data, which often have large and inconsistent variances among years and length bins. Simulating realistic length-composition data is challenging and we have not sufficiently investigated best practices for simulating length-compositions within the DLMtool software.
Reliable and abundant age-composition data are generally not available for the data-limited species for which this framework is designed.
For the above reasons, we propose MPs that make use of only catch and population index data as provisional candidate MPs.

We can divide MPs that make use of catch and/or population index data into four categories: constant catch, index ratio, index slope, and index target MPs:

1. Constant-catch MPs set the recommended catch to some fixed level, typically based on recent or historical catches.
Importantly, constant-catch MPs do not incorporate feedback between the management system and the population---they make the same catch recommendation regardless of trends in the population index. Nonetheless, they represent simple MPs that in many cases represent the status quo or slight modifications of the status quo.

2. Index-ratio MPs base their catch recommendation on a ratio of a population index in one time period compared to another time period---generally a recent period (e.g., last year) compared to a short period before that (e.g. 2--3 years ago).

3. Index-slope MPs fit a regression of population index data over time and make a catch recommendation based on the slope of the regression. They are closely related to index-ratio MPs.

4. Index-target MPs compare recent population index values to the value of the index at a fixed, agreed-upon historical time period to make a catch recommendation that aims to maintain the population index at the fixed historical value.
In this regard, index-target MPs differ subtly but importantly from the index-ratio and index-slope MPs,
which compare recent index values to a moving window of index values as time progresses.

We tailored many of the available empirical MPs to suit BC groundfish stocks (Appendix \@ref(app:MPs)).
For example, most of the available survey data for BC groundfish are collected biennially in any one spatial region.
Therefore, we modified many of the index-based MPs to reflect this reality.
Typically, this involved adding variants of MPs that considered longer time windows when calculating averages or slopes to account for the fact that there was only half the available data compared to an annual survey.
In other cases, we added alternative versions of MPs that encompassed a wider range of control variables.
For example, the Islope MPs [@geromont2015] as originally described and implemented in DLMtool implicitly set the catch recommendation in the first year to 60--80% of the mean catch from the recent five years (assuming a neutral survey index). Since we do not a priori expect BC groundfish stocks to be overfished, we adjusted the relevant control parameters in our provisional MPs to explore a wider range of initial catch recommendations.

### Model-based MPs

In addition to the empirical MPs, we suggest a surplus production (SP) model be considered amongst candidate MPs.
We provisionally include the SP model coded in MSEtool [@huynh_msetool_2019] and based on @fletcher1978 (Appendix \@ref(app:MPs) Section \@ref(sec:mp-sp)) paired with a number of possible harvest control rules (HCRs). We suggest consideration of both @schaefer1954 and @fox1970 production models since it is not clear, until simulation-tested, which will generate better performance statistics for a given stock.
We suggest a weakly informative prior probability distribution be set on the intrinsic rate of population increase $r$, possibly following recommendations such as those in @mcallister2001. Alternative prior probability distributions could be considered tuning parameters in alternative MPs.
The SP model estimates must be paired with an HCR to form a complete MP.
We provisionally suggest a number of HCRs in Appendix \@ref(app:MPs) Section \@ref(sec:mp-model-based).

### Dealing with multiple survey indices within MPs

The vast majority of published data-limited MPs are based on single indices of abundance. This presents a challenge for BC groundfish stocks, since the trawl and hook-and-line fisheries-independent surveys that cover our coast do so on a biennial basis, alternating amongst areas. We suggest the following three possible solutions:

1. Build and test OMs for areas associated with a single index. If these areas are considered simultaneously in a single application of the MP Framework then we suggest comparing performance of the MPs across all areas and, if possible, choosing an MP that performs reasonably well across all areas. This could be accomplished, for example, via a minimax-style solution, where an MP is selected that performs the least poorly across all areas. If MP performance across areas differs substantially, then different MPs may be needed for different areas. A potential problem with this approach is when stocks are larger than the surveyed area and the information captured in a single index does not represent the entire stock.

2. Develop a single index by "stitching" multiple survey indices together, likely with the application of geostatistical spatiotemporal modeling. This is an active area of research [e.g., @shelton2014; @thorson2015; @anderson2019; @anderson2019synopsis] and will likely become more common within the fisheries literature and in stock assessments. While MPs could be developed that average or in some other way combine multiple survey indices, geostatistical modelling is likely to offer a more coherent way of combining survey data from multiple survey protocols or spatial areas.

3. Develop MPs that incorporate multiple survey indices [e.g., @cox2019]. Many existing data-limited MPs, such as those described in this document (Appendix \@ref(app:MPs)), could be modified to incorporate information from multiple indices. For example, separate indices could be fed through the MP algorithms independently and the most biologically precautionary TAC recommendation could be made.

### Reference MPs

In addition to the candidate MPs, it is important to include reference MPs. Provisionally, we suggest the following reference MPs:

* No fishing
* Fishing at *F*~MSY~
* Fishing at 0.75 *F*~MSY~
* Maintaining the current TAC

The purpose of reference MPs is not to explore viable management strategies but to bound the range of expected or possible performance and contextualize whether differences between performance statistics among MPs are meaningful [@punt2016]. For example, the "no fishing" reference MP provides information on maximum possible stock levels and the maximum possible rate of rebuilding under a rebuilding scenario. The "fishing at 0.75 *F*~MSY~" MP illustrates performance under an omniscient manager with perfect information. The MP that maintains the current TAC is included because it illustrates what is likely the near-term default had the framework not been implemented and illustrates the long-term performance expectations given current exploitation levels.

### Including new MPs

The candidate MPs proposed here are a provisional library from which to build.
Data-limited MP development is a rich area of research, likely still in its infancy, and beyond minor adjustments to existing MPs, MP development is not the focus of this document.
More MPs may be developed as part of the application of this framework and will certainly be developed elsewhere in the literature.
MPs used when applying this framework may also be "tuned" to perform well for specific stocks by adjusting algorithm parameters or priors.
The framework presented here has been designed to accommodate new MPs and we expect the library of candidate MPs to grow over time, with the framework providing a means to rigorously test new MPs through the closed-loop simulation process.

## STEP 5: SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES  {#sec:approach5}

Once the objectives, performance metrics, conditioned OMs and MPs are fully specified, a closed-loop simulation framework (Figures \@ref(fig:mse-chart-basic) and \@ref(fig:mse-chart)) can be applied to test relative performance of the MPs with respect to meeting the stated objectives.

We recommend beginning with a satisficing step, where trial simulations are run to screen out MPs that do not meet a basic set of performance criteria [@miller2010].
For example, in our illustrative Rex Sole case study (Appendix \@ref(app:mse-rex)), we screened out MPs that did not achieve a long-term 90% probability of keeping the stock above the LRP (LT LRP $>$ 0.9) and a short-term 80% probability of catch being above recent average catch (STC $>$ 0.8).
We recommend an iterative approach to selecting the satisficing criteria on a stock-by-stock basis.
The purpose is to simplify the decision-making process by focusing on a manageable number of MPs.
If possible, criteria should not be so strict as to filter out MPs with broadly acceptable performance or to filter out almost all MPs.
Similarly, criteria should be strict enough to filter out consistently under-performing MPs.
We recommend selection of satisficing criteria be done iteratively with the technical committee.

DLMtool is designed to follow standard MSE operating procedure (Figure \@ref(fig:mse-chart)).
For each MP, the OM is used to simulate the various data streams required by the MP at each time step, then the population biomass is projected forward under the prescribed MP at each subsequent time step until the projection period is complete.
Performance is then evaluated through calculation of performance statistics in the OM.
DLMtool makes use of the C++ programming language and parallel processing, making the simulations computationally efficient [@carruthers2018].

For each OM-MP combination, multiple replicate projections are run to account for observation and process errors in the data streams.
This is achieved by adding stochastic noise to the data (e.g., indices of abundance) before passing them to the MP.
Coefficients of variation in the data should be consistent with those in historical observations [@rademeyer2007].
We suggest selecting a sufficient number of replicates so that the rank order of MPs across the performance metrics remains consistent regardless of additional replicates [@carruthers_user_2019].
This can be checked by plotting the cumulative performance statistics against the number of replicates, to check that the rank order of MPs with respect to performance statistics will not change with the addition of more replicates (e.g., Figure \@ref(fig:methods-converge)).
In our experience with BC groundfish, the number of required replicates is likely to be at least 100.

(ref:fig-methods-converge) This visualization illustrates the degree to which the closed-loop simulation replicates have reached convergence in the rank order of MP performance.
The y-axis represents the proportion of cumulative replicates that have achieved the performance metric.
The cumulative replicates are shown along the x-axis.
Each panel represents a different performance metric and the colours illustrate the various MPs.
A wiggly line indicates that a given MP is still varying in its performance metric value as more replicates are added.
Lines that are crossing indicate changes to the rank order of MP performance as more replicates are added.
A plot is consistent with convergence when the lines are remaining roughly horizontal and parallel and are not crossing each other towards the right of the panel.

```{r methods-converge, fig.cap="(ref:fig-methods-converge)"}
knitr::include_graphics(here("report/figure/framework-convergence.png"))
```

## STEP 6: PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE  {#sec:approach6}

### Reference and robustness sets

In Step 3 (Section \@ref(sec:approach3)), we recommended dividing OM scenarios into a reference set, which includes a range of plausible alternative OM scenarios that impact results, and a robustness set, which includes a wider range of alternative OM scenarios, which may be less supported by available data, but still potentially impact results [@rademeyer2007].
These authors recommended reducing the set of OM robustness scenarios down to those which yield the most contrasting results from the reference set.
Therefore, before the final MP is selected, the role of the robustness set is to provide a check that the final MP performs well across a more diverse set of OM scenarios.
Poor performance of an MP under one of these OM scenarios may influence a decision-maker to select another MP that performs well under both OM reference and robustness scenarios.

We recommend presenting performance metrics from the reference and robustness sets separately.
For most visualizations, we recommend that the reference set performance metrics are averaged across all OM reference set scenarios.
An exception is the table of performance metrics (e.g., Figure \@ref(fig:methods-tigure)), which is presented in two ways: (i) minimum value of the performance metric across all OM reference set scenarios; and (ii) average value of the performance metric across all OM reference set scenarios.
The first is a "worst-case scenario" approach, whereas the second case integrates across the whole reference set.
We recommend presenting performance metrics from the individual OM robustness-set scenarios separately.
This facilitates visualization of performance of MPs that perform well in the reference set under specific more diverse assumptions [@rademeyer2007].

### Visual presentation of performance metrics

Here we focus on developing a set of provisional visualizations that facilitate comparison of performance metrics across MPs and evaluation of trade-offs amongst them. We have developed an R package gfdlm [@gfdlm] (Appendix \@ref(app:gfdlm)) for generating these visualizations. We expect that some or all of the visualizations will be refined over time as users gain familiarity and specific needs arise. 

First, we suggest a graphical representation of a probability table (Figure \@ref(fig:methods-tigure)) to visualize performance metric results.
This visualization lends itself to a large number of MPs and so works well for displaying results for *all* MPs---not necessarily just satisficed MPs.
By shading the cells according to their underlying performance metric value, the visualization draws the eye to similarities and differences across MPs.
We suggest sorting the rows by performance metrics of particular interest, shading the MP name differently for reference MPs, and highlighting any satisficing criteria by outlining cells that pass satisficing thresholds.
The default colour shading is colour-blind proof, prints accurately in grey scale, and is visually linear in its gradient [@viridis].

Second, we suggest a visualization that lends itself to summarizing performance for a small set of satisficed MPs across OM reference set scenarios (Figure \@ref(fig:methods-dots)).
This figure highlights average performance across OM scenarios (dots) along with the range of performance values seen across OM scenarios (thin lines).
This figure also illustrates the range of performance after dropping the OM scenarios with the highest and lowest of each performance metric values (thicker lines).

We suggest two possible visualizations for highlighting performance-metric trade-offs (Figures \@ref(fig:methods-tradeoff) and \@ref(fig:methods-radar)).
The first is a bivariate dot plot, which lends itself to comparing any two performance metrics.
The second is a "radar" plot (also known as a "kite" or "spider" plot).
Radar plots lend themselves to comparing across many performance metrics, but become difficult to interpret as the number of performance metrics grows (e.g., above six) and one needs to be careful when interpreting these plots because the arrangement of the "spokes" can affect perception of the resulting shapes and humans have been shown to be slower and less accurate at interpreting radial displays of data compared to Cartesian ones [@diehl2010; @feldman2013; @albo2016].

To understand the processes leading to the performance metrics, we recommend that the results from the application of this framework should include visualizations of historical and projected *B*/*B*~MSY~, *F*/*F*~MSY~, and catch.
We suggest two versions: one that lends itself to careful inspection of individual OM scenarios and an understanding of individual replicate behaviour (Figure \@ref(fig:methods-proj)) and another that lends itself to comparing the time series across OM scenarios (Figure \@ref(fig:methods-proj-sens)).
Inspection of these time series can help understand the performance metrics and may also suggest new performance metrics if the existing set fail to capture some important aspect that becomes apparent in the time series.
For example, if some MPs are creating variable TAC recommendations in the initial implementation years this may suggest the need to specify a short-term TAC variability objective and performance metric.
Alternatively, if some MPs are resulting in short-term declines in *B*/*B*~MSY~ that eventually recover by the long-term window, this may suggest the need for a short-term conservation objective and performance metric or the calculation of performance metric probabilities via the minimum probability in any year [e.g., @ices2016criteria].

Visualizing the simulated population-index time series is particularly important when using MPs that rely on the population index (e.g. Figure \@ref(fig:methods-index)).
First, for simple MPs that closely resemble recent management, future simulations of the population index can be used as a type of posterior predictive check [@gelman2014] to ask whether the future simulations look like they are plausible given historical observations of the same population index.
Second, the range of future projected index values can be used as a trigger for re-evaluating an OM.
For example, if a future observed survey index deviates from some quantile of simulated survey index values, the observed state of nature can be considered unlikely given the assumed OMs (sometimes called "exceptional circumstances" [@butterworth2008]).
We note that the cumulative probability of the observed index exceeding some quantile of simulated values increases with time, so such a rule will generate more than the tail probability of false re-evaluation triggers.
For example, even if the OM perfectly captured reality, there is approximately a `r round(pbinom(9, 10, 0.95), 1)` probability (2 in 5 chance) that the observed index will at some point exceed the 95% quantile of simulated index values after 10 years.
Closed-loop simulation itself can be used to establish appropriate triggers with acceptable statistical power [@carruthers_hordyk_2018].

Finally, we suggest two visualizations that illustrate the trade-off between *F*/*F*~MSY~ and *B*/*B*~MSY~ across replicates for the various MPs (Figure \@ref(fig:methods-kobe) and \@ref(fig:methods-worm)).
One is a standard Kobe plot showing *F*/*F*~MSY~ vs. *B*/*B*~MSY~ for the final year of the projection (Figure \@ref(fig:methods-kobe)).
This visualization highlights the parameter space with the highest probability density via quantile kernel-density-estimate contour lines.
The other visualization shows the trajectory of *F*/*F*~MSY~ vs. *B*/*B*~MSY~ through time (Figure \@ref(fig:methods-worm)).
The final year of Figure \@ref(fig:methods-worm) is an alternative representation of Figure \@ref(fig:methods-kobe).

(ref:fig-methods-tigure) This probability table illustrates performance metric values across a number of MPs.
See Section \@ref(sec:approach2) for definitions of the various  performance metrics (columns).
The MPs are ordered by decreasing performance metric values from top to bottom starting with the left-most performance metric and using columns from left to right to break any ties.
The colour shading reflects the underlying numbers and is included to make the differences in the values more readily apparent.
Outlined cells represent MPs that met a given satisficing criterion.
MP names shaded grey represent reference MPs.

```{r methods-tigure, fig.cap="(ref:fig-methods-tigure)", out.width="3.5in"}
knitr::include_graphics(here("report/figure/framework-tigure.png"))
```

(ref:fig-methods-dots)
This visualization summarizes performance of a small number of MPs (e.g., satisficed MPs) across OM scenarios.
Dots represent mean performance across OM scenarios.
Thin lines represent the range of performance across OM scenarios.
Thicker lines represent the range of performance across OM scenarios after dropping the highest and lowest OM scenario within each performance metric.
This visualization can also be used without the line segments to represent performance for individual OM scenarios (e.g., OM robustness scenarios).
Reference MPs are indicated by open circles (True).
Non-reference MPs are indicated by closed circles (False).

```{r methods-dots, fig.cap="(ref:fig-methods-dots)", out.width="\\textwidth"}
knitr::include_graphics(here("report/figure/framework-dot.png"))
```

(ref:fig-methods-tradeoff)
This visualization demonstrates bivariate trade-offs between two performance metrics for a given OM scenario or average across OM scenarios.
The bottom and left axis correspond to two performance metrics and individual dots represent MPs.

```{r methods-tradeoff, fig.cap="(ref:fig-methods-tradeoff)", out.width="4.25in"}
knitr::include_graphics(here("report/figure/framework-trade-off.png"))
```

(ref:fig-methods-radar) This radar plot illustrates performance metric trade-offs amongst a set of MPs.
Each spoke represents a performance metric (Section \@ref(sec:approach2)).
Each line represents an MP.
The position of each line on each spoke represents the probability of that MP achieving that performance metric.
Therefore, lines closer to the outside correspond to MPs that are determined to have a higher probability of achieving that performance metric.
Care should be taken when interpreting these plots because the arrangement of the "spokes" can affect perception of the resulting shapes [@diehl2010; @feldman2013; @albo2016].

```{r methods-radar, fig.cap="(ref:fig-methods-radar)"}
knitr::include_graphics(here("report/figure/framework-radar.png"))
```

(ref:fig-methods-proj) This visualization illustrates historical and projected B/B~MSY~ (where B indicates spawning biomass), F/F~MSY~, and catch for various MPs for a single OM.
Only one MP is shown here.
In applications of the framework, all satisficed MPs would be shown as separate panels.
Dark lines indicate the median value and the darker and lighter shaded ribbons indicate the 50% and 90% quantiles.
Thin gray lines represent illustrative simulation replicates.
The vertical dashed line indicates the last year of the historical period.
The horizontal dashed lines indicate B/B~MSY~ = 1, 0.8, and 0.4 and F/F~MSY~ = 1.
Note that the simulations are mean-unbiased and so the median B/B~MSY~ and F/F~MSY~ are not expected to lie perfectly on the 1 line even if fishing perfectly at F~MSY~.

```{r methods-proj, fig.cap="(ref:fig-methods-proj)", out.width="\\textwidth"}
knitr::include_graphics(here("report/figure/framework-main-projections.png"))
```

(ref:fig-methods-proj-sens)
This visualization highlights sensitivity of the historical conditioned and projected time series across OM scenarios for two hypothetical OM scenarios. 
In applications of the framework with more than two OM scenarios, more lines would be shown.
Only one MP is shown here.
In applications of the framework, all satisficed MPs would be shown as separate panels.
The solid lines correspond to median values and the shaded ribbons correspond to 50% quantiles to indicate variability across replicates.
Whereas Figure \@ref(fig:methods-proj) helps compare across MPs within a given OM scenario, and highlights example replicate performance, this visualization helps compare time series within a given MP across OM scenarios.
This visualization lends itself to multiple rows where each row represents a different MP.
Note that sensitivity of the time series to OM scenarios does not necessarily correspond to sensitivity in terms of rank order of MPs, which is ultimately the important result in a management-oriented approach.

```{r methods-proj-sens, fig.cap="(ref:fig-methods-proj-sens)", out.width="\\textwidth"}
knitr::include_graphics(here("report/figure/framework-sens-projections.png"))
```

(ref:fig-methods-index) This visualization illustrates the historical and projected relative population index values.
The black lines represent example replicates.
The shaded grey region represents the 95% quantile of the simulated index values.
The vertical dashed line represents the last historical year of the observed data (final year of the catch-time series, see Appendix \@ref(app:dlmtool-om)).
In the historical period, the index represents the observed index (usually a survey index).
In cases such as the one illustrated here, the survey is performed every second year.
For index-based MPs, understanding the expected envelope of possible future survey trends is important for evaluating whether the system continues to be consistent with the OM assumptions in the future.
For simple MPs, this visualization is also helpful for assessing whether the simulation is generating future index projections that are consistent with the observed historical trends.

```{r methods-index, fig.cap="(ref:fig-methods-index)", out.width="6in",}
knitr::include_graphics(here("report/figure/framework-index-projections.png"))
```

(ref:fig-methods-kobe) Kobe (B/B~MSY~ vs. F/F~MSY~) plots for the final year of the projections across all replicates.
Dots represent individual replicates.
The contour lines indicate two-dimensional kernel-density-smoothed quantiles, calculated in log space.
For example, the 0.50 contour lines encompass approximately 50% of the replicates. 
The vertical dashed lines shows B/B~MSY~ = 0.4 (left) and 0.8 (right).
The horizontal dashed line shows F/F~MSY~ = 1.
Replicates with values beyond the outer axis limits are shown on the axis limit with an open circle (e.g., bottom right corner).

```{r methods-kobe, fig.cap="(ref:fig-methods-kobe)", out.width="5in"}
knitr::include_graphics(here("report/figure/framework-kobe.png"))
```

(ref:fig-methods-worm) This visualization illustrates the time trajectory of B/B~MSY~ and F/F~MSY~ values summarized across replicates.
The solid line corresponds to the median value.
The vertical dashed lines shows B/B~MSY~ = 0.4 (left) and 0.8 (right).
The horizontal dashed line shows F/F~MSY~ = 1.
Each diamond represents the 50% quantile of B/B~MSY~ (horizontal) and F/F~MSY~ (vertical).
There is one diamond per year of the historical and projection period.
The lines and diamonds change colour over time and specific points in time are illustrated with symbols (first year, last historical year, and last projected year).
For reference, the data underlying the last time slice in this figure are the same as Figure \@ref(fig:methods-kobe).

```{r methods-worm, fig.cap="(ref:fig-methods-worm)", out.width="5in"}
knitr::include_graphics(here("report/figure/framework-worm.png"))
```

\clearpage

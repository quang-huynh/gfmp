# BEST PRACTICES FOR MANAGEMENT PROCEDURE APPROACHES {#sec:best-practices}

@punt2016 reviewed best practices for MSE and identified five key steps in the process (Steps 2--6 below). In large part, the DLMtool software has been designed to allow practitioners to follow these steps [Figure \@ref(fig:mse-chart); @carruthers2018]. We also identify a critical first step (Step 1 below): defining the decision context [@gregory2012; @cox2016].
In most practical applications the steps in the MSE process will be iterative.
For example, objectives or performance metrics may be refined after experience is gained with their performance in simulations or the real world [@delamare1998; @plaganyi2007; @cox2008a; @punt2016; ].
OMs and MPs may also be refined or revised in light of new information or changes to fishing operations or the ecosystem [e.g., @plaganyi2007; @pestal2008].

In this section we provide an overview of the six best practice steps.
We describe how each of the six steps is implemented in the proposed MP Framework in Section \@ref(sec:approach).

(ref:fig-mse-chart) The steps of the MSE process following @punt2016 as implemented in DLMtool. Adapted from @carruthers2018. This figure expands on Figure \@ref(fig:mse-chart-basic).

```{r mse-chart, fig.cap="(ref:fig-mse-chart)", out.width="\\textwidth"}
knitr::include_graphics(here::here("images/mse-chart.pdf"))
```

## STEP 1: DEFINE THE DECISION CONTEXT {#sec:best1}

Key questions to guide defining the decision context for MSE include:

* What is the exact decision to be made?

* What is the time frame for making the decision?

* What are the boundaries on the project and decision?

* What are specific roles and responsibilities of parties involved? Parties include Science, Management, First Nations, industry, academia, and/or non-governmental organizations (NGOs).

* How will the final decision be made? For example, it may be necessary to rank or weight objectives if there are large trade-offs with respect to performance for different objectives.

* How will the process be governed? For example, how will aceptability of trade-offs be determined? How will meetings be facilitated? How will consultation be managed? Failures in governence of the decision-making process can lead to a less succesful decisions in terms of acceptance and compliance [@armitage2019].

Definition of the decision context is the role of managers, stakeholders, First Nations, and other key interested parties.

## STEP 2: SELECTION OF OBJECTIVES AND PERFORMANCE METRICS {#sec:best2}

Clear management and fishery objectives and the performance metrics that measure them must be identified. Objectives may initially be high level and "strategic" (e.g., achieve sustainable fisheries, maintain economic prosperity, maintain cultural access) but these must be converted into operational "tactical" objectives that can be expressed as quantitative performance metrics [@hilborn2007; @delamare1998; @punt2016].
Fully quantified objectives include a metric, the desired probability of success, and a time frame to achieve the objective (e.g., probability of maintaining the stock above the LRP is greater than 0.95 [19 times out of 20], in each and every year of a 50-year period).

Since the properties of the underlying system represented by the OM are known exactly, a wide range of biological and economic metrics can be calculated from the OM [@carruthers2018].
However, having too many performance metrics can make the final decision process complex. Performance metrics should be chosen so they can be understood by decision-makers and participants, and to facilitate a tractable decision-making environment [@punt2016].

Objectives should be developed with the participation of managers, stakeholders, First Nations, and other interested parties [e.g., @mapstone2008; @plaganyi2007; @cox2008a].
@hilborn2007 identified four categories of fishery objectives: biological, economic, social and political, noting that many conflicts in fisheries in fact arise from conflicting objectives.
Within each of these categories, different resource users and interest groups will place value on different components, leading to inevitable trade-offs.
For example, while all resource users may place higher value on increased biological production, aboriginal users may prefer more stable access to yields and increased participation [@plaganyi2013], while recreational users may prefer lower yields and larger trophy fish [@hilborn2007].
Achieving agreement on the list of objectives and performance metrics, especially with multiple user groups, can be time-consuming and should be iterative, as participants build familiarity with each other and the process.
During Step 1 (Section \@ref(sec:best1)), attention should be given to governance of the process to ensure that there is meaningful participation by different groups and that participants can agree on the final set of objectives to adequately characterize the main trade-offs.

## STEP 3: SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS {#sec:best3}

Uncertainties inherent in the underlying system are represented in the OM. Uncertainty in the OMs may be related to: the biology of the stock (e.g., growth, natural mortality, recruitment, migration); the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear); the observation process (e.g., bias or imprecision in survey data or age/length composition data); and/or the implementation process (e.g., exceeding catch limits) [@carruthers2018].

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured within a single OM by expressing distributions for these parameters.
However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in a single OM.
Therefore, best practice recommends dividing MSE trials into a "reference set", using a core set of OMs that include the most important uncertainties (e.g., depletion of the stock or range of natural mortality values), and a "robustness set", representing other plausible OM formulations that represent alternative structural hypotheses [@rademeyer2007].
These authors recommend  that the reference set of OMs include the most important uncertainties, which are both highly plausible and have major impacts on results.
While there is no established objective means for selecting OMs for the reference set, @punt2016 suggest that best practice start from a common set of factors which commonly have a large impact on MSE performance due to uncertainties.
They provide a list of factors which commonly have a large impact on MSE performance due to uncertainty (their Table 3) and suggests that, at a minimum, MSE processes should consider: parameter uncertainty (related to productivity and stock size); process uncertainty; and observation error.
Interactions among uncertainties may be considered by evaluating all combinations of uncertainty factors [e.g., @rademeyer2006a].
However, this may not always be computationally possible, and it is more common to select ‘base’ levels for each factor and then  develop alternative OMs which vary one (or more than one) factor in turn [@punt2016].
An iterative approach may be required at the beginning to determine which combinations produce the largest differences in results.

Once an agreed-upon reference set of OMs has been determined, a wider range of OMs (the robustness set) should be developed to capture a wider range of uncertainties that may be less plausible but should nonetheless be explored [@rademeyer2007].
These may include effects related to environmental change (e.g., time-varying mortality, climate-driven recruitment, predator-prey relationships); structural representation of population dynamics (e.g., form of the stock-recruit relationship); or fleet dynamics (e.g., selectivity).
@punt2016 also note that, in some cases, where the data used to parameterize the OM are in conflict (e.g., two indices of abundance are in conflict), the best practice may be to develop alternative OMs based on the different data sources.
Other uncertainties in past reliability or future availability of data may also be captured in the robustness set [@rademeyer2007].

Reference and robustness sets may be selected through an iterative process examining the impact of uncertainties on the MSE performance.
For example, @rademeyer2006b evaluated 28 preliminary robustness tests for a South African hake case study but discontinued tests that produced results very similar to the reference set trials.
OMs may be weighted, where weightings may be based on qualitative plausibility criteria [@butterworth1996], or may be quantitative based on model-selection criteria based on fits to data (e.g., AIC).
However, @punt2016 urged caution in using model-selection critera to weight OMs unless there was very high confidence in the reliability of the likelihood function, which is unlikely to be the case in data-limited situations.
Placing less plausible OMs into the reference set may somewhat alleviate the need to consider weighting of OMs.

Ideally, OMs should be conditioned on real data to ensure they can reproduce historical observations [e.g., @cox2008a; @forrest2018].
In data-limited cases without reliable historical observations, this may not be possible.
In these cases, best practice would be to develop a set of OMs that differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

Development of OMs is principally the responsibility of Science, although input from stakeholders, First Nations and other parties is desirable, especially with respect to identifying key uncertainties and ensuring plausibility of the OMs.

## STEP 4: IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES {#sec:best4}

The scientific literature now reports many MPs for data-limited fisheries, more than 80 of which have been integrated into the DLMtool software [@carruthers2016; @carruthers2018].
Management procedures for fisheries managed by catch limits are generally either model-based, where data are integrated into a stock assessment model and outputs are used to calculate catch limits, or empirical, where data are used in an algorithm to directly determine the catch limit (e.g., adjustment of catch based on change in index of abundance) [@punt2016].
Empirical MPs can make use of a variety of data types including catch, population indices, fish lengths, and fish ages.

Empirical MPs take data sampled from the system, such as a survey index, apply an algorithm, and make a catch recommendation.
An example is the "Iratio" MP [@ices2012; @jardim2015], where the mean survey index value from the last two years is divided by the mean survey index value three to five years before present.
This provides a ratio indicating whether the survey has increased or decreased, which is then multiplied by the previous year's catch to generate a new catch recommendation.
If the survey index has been trending up, then the catch recommendation will increase, and vice versa.
Model-based MPs fit a statistical population model (e.g., surplus production model) to observed data to estimate biological reference points and stock biomass.
These are then incorporated into a harvest control rule (e.g., Figure \@ref(fig:pa-illustration)) to determine the catch limit for the following year.

Given the large number of MP options available, a screening step is desirable.
For example, MPs that do not return a catch limit (e.g., spatial closures or effort-based MPs) can be immediately screened out if management requires a catch limit.
Also, unless the decision context involves considering the value of collecting new information, it is important to test MPs for which information or data are available [@punt2016].
For example, MPs that rely on age-composition data or an estimate of current depletion may not be feasible for many data-limited BC groundfish stocks.
While it is important to work with a manageable set of MPs, it is also important not to screen too aggressively, to make sure good candidate MPs are not screened out early on.

In general, identification of available MPs is the role of Science. Managers, stakeholders and First Nations should be involved in identifying desirable MPs and provide input on feasibility of implementing some MPs and their likely success in terms of acceptance and compliance [@armitage2019].

## STEP 5: SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES {#sec:best5}

Once the OM and MPs are fully specified, the closed-loop simulation replicates can be run, following the process illustrated in Figure \@ref(fig:mse-chart).
Critically, the simulations include feedback between the OM and the MP, where the OM generates data at each time step, which is used to apply the MP, which generates a catch recommendation, which is removed from the OM, which generates the next time step of data, and so forth until the projection period is complete.

Typically, a large number of replicate simulations are run for each OM-MP combination.
Replicates may differ in terms of OM process error, observation errors and random draws from ranges of OM parameters, meaning that each replicate provides a different set of simulated data to the MPs.
The number of replicates should be selected to ensure that performance metrics can be calculated with adequate precision [@punt2016], which can be indicated by MPs being consistently ranked in the same order regardless of additional replicates [@carruthers2018].
The MSE should output enough information to calculate performance metrics for the MPs, and also to evaluate the behaviour and performance of the MSE itself (e.g., whether all trials converged, ranges of OM parameter values, and trajectories of key OM variables such as biomass and catch).

There may be a need to reduce the number of candidate MPs to a manageable set.
Analysts can screen out MPs that do not meet a basic set of requirements for a broad range of stocks (e.g., MPs that result in a high probability of stocks being below the LRP).
Such a procedure of screening out poorly performing MPs has been termed "satisficing" [@miller2010], where MPs must meet a minimum-defined standard to be accepted.
Satisficing criteria may be used at the screening stage and can also be used at the final MP selection stage to help streamline the decision-making process.
Satisficing criteria may be less strict at the preliminary screening stage, to ensure that potentially successful MPs are not screened out of the process too early.

Running the simulations is the role of Science.
Feedback from managers, stakeholders and First Nations should be sought throughout the process, to enable iterative refinement of the models and outputs [e.g., @cox2008a].

## STEP 6: PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE {#sec:best6}

Selection of the "best" MP involves addressing trade-offs (e.g., between conservation and economic performance metrics), and therefore is the purview of managers, stakeholders, First Nations, and interested parties [@punt2016].
Ultimately, selection of the best MP may be a subjective process, depending on the magnitude of trade-offs.
It may be necessary to rank performance metrics in order of priority before the process starts.
The role of Science in this step is to ensure that results are clearly presented to decision-makers.
Ideally this should include presentation of graphical outputs that enable clear comparison of MPs with respect to performance metrics and trade-offs [@punt2017].

Two basic approaches may be used in selecting the final MP: satisficing and trade-off evaluation, where satisficing involves setting minimum performance standards (described in Section \@ref(sec:best5)) and trade-off evaluation involves decision-makers and stakeholders finding a balance among competing performance metrics [@punt2017].
We already described an early satisficing step to screen out poorly performing MPs.
A second satisficing step may be used towards the end of the process, to further screen out MPs that do not meet a minimum standard and to simplify the decision-making environment [@miller2010].
After this, MP selection may move to a trading-off stage with a final reduced set of MPs.
An iterative process may also be required, where MPs and/or OMs are refined following examination of results [e.g., @cox2008a].
In cases where there is a reference and robustness set of OMs, OMs can be weighted on the basis of plausibility, although this may require a qualitative, expert-driven approach and may not be straightforward [@punt2016].

@carruthers2018 also discuss a final step (Step 7 in Figure \@ref(fig:mse-chart)), which is formal review of the selected MP once it has been implemented with real data.
Formal review includes evaluation of whether the MP is performing as expected.
For example, this could be done by comparing whether real relative abundance indices follow similar trajectories to those predicted by the OMs under the selected MP.
In this document, we do not demonstrate this formal review, but recognize that ongoing review of the performance of MPs following their application is a critical component of MSE, where OMs and MPs may be continuously refined as new data become available [@cox2008a; @carruthers_hordyk_2018].

Selection of the final MP should ideally result from careful specification of the objectives and performance metrics.
In cases where a trade-off remains, or multiple MPs achieve sufficient performance, it is the role of managers---with input from stakeholders and First Nations and advice from Science---to select the final MP.
